/*
 * Copyright 2017-2018 Dgraph Labs, Inc.
 *
 * This file is available under the Apache License, Version 2.0,
 * with the Commons Clause restriction.
 */

package posting

import (
	"bytes"
	"context"
	"encoding/base64"
	"math"
	"sort"
	"sync/atomic"
	"time"

	"github.com/dgraph-io/badger"
	"github.com/dgraph-io/dgo/protos/api"
	"github.com/dgraph-io/dgraph/protos/intern"
	"github.com/dgraph-io/dgraph/x"
)

var (
	ErrTsTooOld = x.Errorf("Transaction is too old")
)

func (t *Txn) SetAbort() {
	atomic.StoreUint32(&t.shouldAbort, 1)
}

func (t *Txn) ShouldAbort() bool {
	if t == nil {
		return false
	}
	return atomic.LoadUint32(&t.shouldAbort) > 0
}

func (t *Txn) AddDelta(key []byte, p *intern.Posting, checkConflict bool) {
	t.Lock()
	defer t.Unlock()
	t.deltas = append(t.deltas, delta{key: key, posting: p, checkConflict: checkConflict})
}

func (t *Txn) Fill(ctx *api.TxnContext) {
	t.Lock()
	defer t.Unlock()
	ctx.StartTs = t.StartTs
	for i := t.nextKeyIdx; i < len(t.deltas); i++ {
		d := t.deltas[i]
		if d.checkConflict {
			// Instead of taking a fingerprint of the keys, send the whole key to Zero. So, Zero can
			// parse the key and check if that predicate is undergoing a move, hence avoiding #2338.
			k := base64.StdEncoding.EncodeToString(d.key)
			ctx.Keys = append(ctx.Keys, k)
		}
	}
	t.nextKeyIdx = len(t.deltas)
}

// Don't call this for schema mutations. Directly commit them.
// TODO: Simplify this function. All it should be doing is to store the deltas, and not try to
// generate state. The state should only be generated by rollup, which in turn should look at the
// last Snapshot Ts, to determine how much of the PL to rollup. We only want to roll up the deltas,
// with commit ts <= snapshot ts, and not above.
func (tx *Txn) CommitMutations(ctx context.Context, commitTs uint64) error {
	tx.Lock()
	defer tx.Unlock()

	txn := pstore.NewTransactionAt(commitTs, true)
	defer txn.Discard()
	// Sort by keys so that we have all postings for same pl side by side.
	sort.SliceStable(tx.deltas, func(i, j int) bool {
		return bytes.Compare(tx.deltas[i].key, tx.deltas[j].key) < 0
	})
	var prevKey []byte
	var pl *intern.PostingList
	var plist *List
	var err error
	i := 0
	for i < len(tx.deltas) {
		d := tx.deltas[i]
		if !bytes.Equal(prevKey, d.key) {
			plist, err = Get(d.key)
			if err != nil {
				return err
			}
			if plist.AlreadyCommitted(tx.StartTs) {
				// Delta already exists, so skip the key
				// There won't be any race from lru eviction, because we don't
				// commit in memory unless we write delta to disk.
				i++
				for i < len(tx.deltas) && bytes.Equal(tx.deltas[i].key, d.key) {
					i++
				}
				continue
			}
			pl = new(intern.PostingList)
		}
		prevKey = d.key
		var meta byte
		if d.posting.Op == Del && bytes.Equal(d.posting.Value, []byte(x.Star)) {
			pl.Postings = pl.Postings[:0]
			// Indicates that this is the full posting list.
			meta = BitEmptyPosting
		} else {
			midx := sort.Search(len(pl.Postings), func(idx int) bool {
				mp := pl.Postings[idx]
				return d.posting.Uid <= mp.Uid
			})
			if midx >= len(pl.Postings) {
				pl.Postings = append(pl.Postings, d.posting)
			} else if pl.Postings[midx].Uid == d.posting.Uid {
				// Replace
				pl.Postings[midx] = d.posting
			} else {
				pl.Postings = append(pl.Postings, nil)
				copy(pl.Postings[midx+1:], pl.Postings[midx:])
				pl.Postings[midx] = d.posting
			}
			meta = BitDeltaPosting
		}

		// delta postings are pointers to the postings present in the Pl present in lru.
		// commitTs is accessed using RLock & atomics except in marshal so no RLock.
		// TODO: Fix this hack later
		plist.Lock()
		val, err := pl.Marshal()
		plist.Unlock()
		x.Check(err)
		if err = txn.SetWithMeta([]byte(d.key), val, meta); err == badger.ErrTxnTooBig {
			if err := txn.CommitAt(commitTs, nil); err != nil {
				return err
			}
			txn = pstore.NewTransactionAt(commitTs, true)
			if err := txn.SetWithMeta([]byte(d.key), val, meta); err != nil {
				return err
			}
		} else if err != nil {
			return err
		}
		i++
	}
	if err := txn.CommitAt(commitTs, nil); err != nil {
		return err
	}
	return tx.commitMutationsMemory(ctx, commitTs)
}

func (tx *Txn) CommitMutationsMemory(ctx context.Context, commitTs uint64) error {
	tx.Lock()
	defer tx.Unlock()
	return tx.commitMutationsMemory(ctx, commitTs)
}

func (tx *Txn) commitMutationsMemory(ctx context.Context, commitTs uint64) error {
	for _, d := range tx.deltas {
		plist, err := Get(d.key)
		if err != nil {
			return err
		}
		err = plist.CommitMutation(ctx, tx.StartTs, commitTs)
		for err == ErrRetry {
			time.Sleep(5 * time.Millisecond)
			plist, err = Get(d.key)
			if err != nil {
				return err
			}
			err = plist.CommitMutation(ctx, tx.StartTs, commitTs)
		}
		if err != nil {
			return err
		}
	}
	return nil
}

func (tx *Txn) AbortMutations(ctx context.Context) error {
	tx.Lock()
	defer tx.Unlock()
	for _, d := range tx.deltas {
		plist, err := Get([]byte(d.key))
		if err != nil {
			return err
		}
		err = plist.AbortTransaction(ctx, tx.StartTs)
		for err == ErrRetry {
			time.Sleep(5 * time.Millisecond)
			plist, err = Get(d.key)
			if err != nil {
				return err
			}
			err = plist.AbortTransaction(ctx, tx.StartTs)
		}
		if err != nil {
			return err
		}
	}
	atomic.StoreUint32(&tx.shouldAbort, 1)
	return nil
}

func unmarshalOrCopy(plist *intern.PostingList, item *badger.Item) error {
	// It's delta
	val, err := item.Value()
	if err != nil {
		return err
	}
	if len(val) == 0 {
		// empty pl
		return nil
	}
	// Found complete pl, no needn't iterate more
	if item.UserMeta()&BitUidPosting != 0 {
		plist.Uids = make([]byte, len(val))
		copy(plist.Uids, val)
	} else if len(val) > 0 {
		x.Check(plist.Unmarshal(val))
	}
	return nil
}

func getNew(key []byte, pstore *badger.ManagedDB) (*List, error) {
	l := new(List)
	l.key = key
	l.mutationMap = make(map[uint64]*intern.PostingList)
	l.activeTxns = make(map[uint64]struct{})
	l.plist = new(intern.PostingList)
	txn := pstore.NewTransactionAt(math.MaxUint64, false)
	defer txn.Discard()

	item, err := txn.Get(key)
	if err == badger.ErrKeyNotFound {
		return l, nil
	}
	if err != nil {
		return l, err
	}
	if item.UserMeta()&BitCompletePosting > 0 {
		err = unmarshalOrCopy(l.plist, item)
		l.minTs = item.Version()
		l.commitTs = item.Version()
	} else {
		iterOpts := badger.DefaultIteratorOptions
		iterOpts.AllVersions = true
		it := txn.NewIterator(iterOpts)
		defer it.Close()
		it.Seek(key)
		l, err = ReadPostingList(key, it)
	}

	if err != nil {
		return l, err
	}

	l.onDisk = 1
	l.Lock()
	size := l.calculateSize()
	l.Unlock()
	x.BytesRead.Add(int64(size))
	atomic.StoreInt32(&l.estimatedSize, size)
	return l, nil
}

type bTreeIterator struct {
	keys    [][]byte
	idx     int
	reverse bool
	prefix  []byte
}

func (bi *bTreeIterator) Next() {
	bi.idx++
}

func (bi *bTreeIterator) Key() []byte {
	x.AssertTrue(bi.Valid())
	return bi.keys[bi.idx]
}

func (bi *bTreeIterator) Valid() bool {
	return bi.idx < len(bi.keys)
}

func (bi *bTreeIterator) Seek(key []byte) {
	cont := func(key []byte) bool {
		if !bytes.HasPrefix(key, bi.prefix) {
			return false
		}
		bi.keys = append(bi.keys, key)
		return true
	}
	if !bi.reverse {
		btree.AscendGreaterOrEqual(key, cont)
	} else {
		btree.DescendLessOrEqual(key, cont)
	}
}

type TxnPrefixIterator struct {
	btreeIter  *bTreeIterator
	badgerIter *badger.Iterator
	prefix     []byte
	reverse    bool
	curKey     []byte
	userMeta   byte // userMeta stored as part of badger item, used to skip empty PL in has query.
}

func NewTxnPrefixIterator(txn *badger.Txn,
	iterOpts badger.IteratorOptions, prefix, key []byte) *TxnPrefixIterator {
	x.AssertTrue(iterOpts.PrefetchValues == false)
	txnIt := new(TxnPrefixIterator)
	txnIt.reverse = iterOpts.Reverse
	txnIt.prefix = prefix
	txnIt.btreeIter = &bTreeIterator{
		reverse: iterOpts.Reverse,
		prefix:  prefix,
	}
	txnIt.btreeIter.Seek(key)
	// Create iterator only after copying the keys from btree, or else there could
	// be race after creating iterator and before reading btree. Some keys might end up
	// getting deleted and iterator won't be initialized with new memtbales.
	txnIt.badgerIter = txn.NewIterator(iterOpts)
	txnIt.badgerIter.Seek(key)
	txnIt.Next()
	return txnIt
}

func (t *TxnPrefixIterator) Valid() bool {
	return len(t.curKey) > 0
}

func (t *TxnPrefixIterator) compare(key1 []byte, key2 []byte) int {
	if !t.reverse {
		return bytes.Compare(key1, key2)
	}
	return bytes.Compare(key2, key1)
}

func (t *TxnPrefixIterator) Next() {
	if len(t.curKey) > 0 {
		// Avoid duplicate keys during merging.
		for t.btreeIter.Valid() && t.compare(t.btreeIter.Key(), t.curKey) <= 0 {
			t.btreeIter.Next()
		}
		for t.badgerIter.ValidForPrefix(t.prefix) &&
			t.compare(t.badgerIter.Item().Key(), t.curKey) <= 0 {
			t.badgerIter.Next()
		}
	}

	t.userMeta = 0 // reset it.
	if !t.btreeIter.Valid() && !t.badgerIter.ValidForPrefix(t.prefix) {
		t.curKey = nil
		return
	} else if !t.badgerIter.ValidForPrefix(t.prefix) {
		t.storeKey(t.btreeIter.Key())
		t.btreeIter.Next()
	} else if !t.btreeIter.Valid() {
		t.userMeta = t.badgerIter.Item().UserMeta()
		t.storeKey(t.badgerIter.Item().Key())
		t.badgerIter.Next()
	} else { // Both are valid
		if t.compare(t.btreeIter.Key(), t.badgerIter.Item().Key()) < 0 {
			t.storeKey(t.btreeIter.Key())
			t.btreeIter.Next()
		} else {
			t.userMeta = t.badgerIter.Item().UserMeta()
			t.storeKey(t.badgerIter.Item().Key())
			t.badgerIter.Next()
		}
	}
}

func (t *TxnPrefixIterator) UserMeta() byte {
	return t.userMeta
}

func (t *TxnPrefixIterator) storeKey(key []byte) {
	if cap(t.curKey) < len(key) {
		t.curKey = make([]byte, 2*len(key))
	}
	t.curKey = t.curKey[:len(key)]
	copy(t.curKey, key)
}

func (t *TxnPrefixIterator) Key() []byte {
	return t.curKey
}

func (t *TxnPrefixIterator) Close() {
	t.badgerIter.Close()
}
