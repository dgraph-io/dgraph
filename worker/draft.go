/*
 * Copyright (C) 2017 Dgraph Labs, Inc. and Contributors
 *
 * This program is free software: you can redistribute it and/or modify
 * it under the terms of the GNU Affero General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU Affero General Public License for more details.
 *
 * You should have received a copy of the GNU Affero General Public License
 * along with this program.  If not, see <http://www.gnu.org/licenses/>.
 */

package worker

import (
	"bytes"
	"encoding/binary"
	"errors"
	"fmt"
	"log"
	"math/rand"
	"sync"
	"time"

	"google.golang.org/grpc"

	"github.com/coreos/etcd/raft"
	"github.com/coreos/etcd/raft/raftpb"
	"golang.org/x/net/context"

	"github.com/dgraph-io/dgraph/posting"
	"github.com/dgraph-io/dgraph/protos"
	"github.com/dgraph-io/dgraph/raftwal"
	"github.com/dgraph-io/dgraph/schema"
	"github.com/dgraph-io/dgraph/types"
	"github.com/dgraph-io/dgraph/x"
)

const (
	proposalMutation   = 0
	proposalReindex    = 1
	proposalMembership = 2
)

type clusterOp int

const (
	ADD_GROUPS    clusterOp = 1
	REMOVE_GROUPS clusterOp = 2
	REMOVE_SERVER clusterOp = 3
)

var (
	errorNodeIDExists        = errors.New("Error Node ID already exists in the cluster")
	errorInvalidToNodeId     = errors.New("Error Invalid Recipient Node ID")
	errorNodeIDBanned        = errors.New("Error Node with given ID has been removed/banned")
	errorStoppedServingGroup = errors.New("Node has stopped serving the group")
	errorNoPeer              = errors.New("Peer doesn't exist")
)

// peerPool stores the peers per node and the addresses corresponding to them.
// We then use pool() to get an active connection to those addresses.
type peerPool struct {
	sync.RWMutex
	peers map[uint64]string
}

func (p *peerPool) Get(id uint64) string {
	p.RLock()
	defer p.RUnlock()
	return p.peers[id]
}

func (p *peerPool) Set(id uint64, addr string) {
	p.Lock()
	defer p.Unlock()
	p.peers[id] = addr
}

type proposals struct {
	sync.RWMutex
	ids map[uint64]chan error
}

func (p *proposals) Store(pid uint64, ch chan error) bool {
	p.Lock()
	defer p.Unlock()
	if _, has := p.ids[pid]; has {
		return false
	}
	p.ids[pid] = ch
	return true
}

func (p *proposals) Done(pid uint64, err error) {
	var ch chan error
	p.Lock()
	ch, has := p.ids[pid]
	if has {
		delete(p.ids, pid)
	}
	p.Unlock()
	if !has {
		return
	}
	ch <- err
}

type sendmsg struct {
	to   uint64
	data []byte
}

type ClusterConfChanges struct {
	Id    uint64
	Gids  []uint32
	Op    clusterOp
	OpStr string
}

func (cc *ClusterConfChanges) Valid() bool {
	switch cc.OpStr {
	case "ADD_GROUPS":
		cc.Op = ADD_GROUPS
	case "REMOVE_GROUPS":
		cc.Op = REMOVE_GROUPS
	case "REMOVE_SERVER":
		cc.Op = REMOVE_SERVER
	}
	if cc.Op == ADD_GROUPS || cc.Op == REMOVE_GROUPS {
		return len(cc.Gids) > 0
	}
	return cc.Op == REMOVE_SERVER
}

type node struct {
	x.SafeMutex

	// SafeMutex is for fields which can be changed after init.
	_confState *raftpb.ConfState
	_raft      raft.Node

	// Fields which are never changed after init.
	cfg         *raft.Config
	applyCh     chan raftpb.Entry
	ctx         context.Context
	stop        chan struct{} // to send the stop signal to Run
	done        chan struct{} // to check whether node is running or not
	gid         uint32
	id          uint64
	messages    chan sendmsg
	peers       peerPool
	props       proposals
	raftContext *protos.RaftContext
	store       *raft.MemoryStorage
	wal         *raftwal.Wal

	canCampaign bool
	// applied is used to keep track of the applied RAFT proposals.
	// The stages are proposed -> committed (accepted by cluster) ->
	// applied (to PL) -> synced (to RocksDB).
	applied x.WaterMark
}

// SetRaft would set the provided raft.Node to this node.
// It would check fail if the node is already set.
func (n *node) SetRaft(r raft.Node) {
	n.Lock()
	defer n.Unlock()
	x.AssertTrue(n._raft == nil)
	n._raft = r
}

// Raft would return back the raft.Node stored in the node.
func (n *node) Raft() raft.Node {
	n.RLock()
	defer n.RUnlock()
	return n._raft
}

// SetConfState would store the latest ConfState generated by ApplyConfChange.
func (n *node) SetConfState(cs *raftpb.ConfState) {
	n.Lock()
	defer n.Unlock()
	n._confState = cs
	fmt.Printf("conf %v\n", cs.Nodes)
}

// ConfState would return the latest ConfState stored in node.
func (n *node) ConfState() *raftpb.ConfState {
	n.RLock()
	defer n.RUnlock()
	return n._confState
}

func newNode(gid uint32, id uint64, myAddr string) *node {
	fmt.Printf("Node with GroupID: %v, ID: %v\n", gid, id)

	peers := peerPool{
		peers: make(map[uint64]string),
	}
	props := proposals{
		ids: make(map[uint64]chan error),
	}

	store := raft.NewMemoryStorage()
	rc := &protos.RaftContext{
		Addr:  myAddr,
		Group: gid,
		Id:    id,
	}

	n := &node{
		ctx:   context.Background(),
		id:    id,
		gid:   gid,
		store: store,
		cfg: &raft.Config{
			ID:              id,
			ElectionTick:    10,
			HeartbeatTick:   1,
			Storage:         store,
			MaxSizePerMsg:   4096,
			MaxInflightMsgs: 256,
		},
		applyCh:     make(chan raftpb.Entry, numPendingMutations),
		peers:       peers,
		props:       props,
		raftContext: rc,
		messages:    make(chan sendmsg, 1000),
		stop:        make(chan struct{}),
		done:        make(chan struct{}),
	}
	n.applied = x.WaterMark{Name: fmt.Sprintf("Committed: Group %d", n.gid)}
	n.applied.Init()

	return n
}

func (n *node) Connect(pid uint64, addr string) {
	for n == nil {
		// Sometimes this function causes a panic. My guess is that n is sometimes still uninitialized.
		time.Sleep(time.Second)
	}
	if pid == n.id {
		return
	}
	if paddr := n.peers.Get(pid); paddr == addr {
		return
	}
	pools().connect(addr)
	n.peers.Set(pid, addr)
}

func (n *node) AddToCluster(ctx context.Context, pid uint64) error {
	addr := n.peers.Get(pid)
	x.AssertTruef(len(addr) > 0, "Unable to find conn pool for peer: %d", pid)
	rc := &protos.RaftContext{
		Addr:  addr,
		Group: n.raftContext.Group,
		Id:    pid,
	}
	rcBytes, err := rc.Marshal()
	x.Check(err)
	// Ideally we should wait till membership information has propagated
	// for both add cluster and remove from cluster to avoid issues.
	return n.proposeConfChange(ctx, &raftpb.ConfChange{
		Type:    raftpb.ConfChangeAddNode,
		NodeID:  pid,
		Context: rcBytes,
	})
}

func (n *node) RemoveFromCluster(ctx context.Context, pid uint64) error {
	peerId, has := groups().Peer(n.gid, pid)
	// Try leadership transfer if removed node is the leader
	if has && n.Raft().Status().Lead == pid {
		n.Raft().TransferLeadership(n.ctx, pid, peerId)
		select {
		case <-n.ctx.Done(): // time out
			if n.AmLeader() {
				return x.Errorf("context timed out while transfering leadership")
			}
		case <-time.After(1 * time.Second):
			if n.AmLeader() {
				return x.Errorf("Timed out transfering leadership")
			}
		}
	}
	return n.proposeConfChange(ctx, &raftpb.ConfChange{
		Type:   raftpb.ConfChangeRemoveNode,
		NodeID: pid,
	})
}

type header struct {
	proposalId uint32
	msgId      uint16
}

func (h *header) Length() int {
	return 6 // 4 bytes for proposalId, 2 bytes for msgId.
}

func (h *header) Encode() []byte {
	result := make([]byte, h.Length())
	binary.LittleEndian.PutUint32(result[0:4], h.proposalId)
	binary.LittleEndian.PutUint16(result[4:6], h.msgId)
	return result
}

func (h *header) Decode(in []byte) {
	h.proposalId = binary.LittleEndian.Uint32(in[0:4])
	h.msgId = binary.LittleEndian.Uint16(in[4:6])
}

var slicePool = sync.Pool{
	New: func() interface{} {
		return make([]byte, 256<<10)
	},
}

func (n *node) proposeConfChange(ctx context.Context, cc *raftpb.ConfChange) error {
	cc.ID = rand.Uint64()
	if err := n.Raft().ProposeConfChange(ctx, *cc); err != nil {
		return x.Wrapf(err, "While removing node")
	}

	che := make(chan error, 1)
	n.props.Store(cc.ID, che)

	x.Trace(ctx, "Waiting for the conf change proposal")

	select {
	case err := <-che:
		x.TraceError(ctx, err)
		return err
	case <-ctx.Done():
		return ctx.Err()
	}
}

func (n *node) ProposeAndWait(ctx context.Context, proposal *protos.Proposal) error {
	if n.Raft() == nil {
		return x.Errorf("RAFT isn't initialized yet")
	}

	// Do a type check here if schema is present
	// In very rare cases invalid entries might pass through raft, which would
	// be persisted, we do best effort schema check while writing
	if proposal.Mutations != nil {
		for _, edge := range proposal.Mutations.Edges {
			if typ, err := schema.State().TypeOf(edge.Attr); err != nil {
				continue
			} else if err := validateAndConvert(edge, typ); err != nil {
				return err
			}
		}
		for _, schema := range proposal.Mutations.Schema {
			if err := checkSchema(schema); err != nil {
				return err
			}
		}
	}

	che := make(chan error, 1)
	for {
		id := rand.Uint64()
		if n.props.Store(id, che) {
			proposal.Id = id
			break
		}
	}

	slice := slicePool.Get().([]byte)
	if len(slice) < proposal.Size() {
		slice = make([]byte, proposal.Size())
	}
	defer slicePool.Put(slice)

	upto, err := proposal.MarshalTo(slice)
	if err != nil {
		return err
	}
	proposalData := make([]byte, upto+1)
	// Examining first byte of proposalData will quickly tell us what kind of
	// proposal this is.
	if proposal.RebuildIndex != nil {
		proposalData[0] = proposalReindex
	} else if proposal.Mutations != nil {
		proposalData[0] = proposalMutation
	} else if proposal.Membership != nil {
		proposalData[0] = proposalMembership
	} else {
		x.Fatalf("Unknown proposal")
	}
	copy(proposalData[1:], slice)

	if err = n.Raft().Propose(ctx, proposalData); err != nil {
		return x.Wrapf(err, "While proposing")
	}

	// Wait for the proposal to be committed.
	if proposal.Mutations != nil {
		x.Trace(ctx, "Waiting for the proposal: mutations.")
	} else if proposal.Membership != nil {
		x.Trace(ctx, "Waiting for the proposal: membership update.")
	} else if proposal.RebuildIndex != nil {
		x.Trace(ctx, "Waiting for the proposal: RebuildIndex")
	} else {
		x.Fatalf("Unknown proposal")
	}

	select {
	case err = <-che:
		x.TraceError(ctx, err)
		return err
	case <-ctx.Done():
		return ctx.Err()
	}
}

func (n *node) send(m raftpb.Message) {
	x.AssertTruef(n.id != m.To, "Seding message to itself")
	data, err := m.Marshal()
	x.Check(err)
	if m.Type != raftpb.MsgHeartbeat && m.Type != raftpb.MsgHeartbeatResp {
		fmt.Printf("\t\tSENDING: %v %v-->%v\n", m.Type, m.From, m.To)
	}
	select {
	case n.messages <- sendmsg{to: m.To, data: data}:
		// pass
	default:
		x.Fatalf("Unable to push messages to channel in send")
	}
}

func (n *node) batchAndSendMessages() {
	batches := make(map[uint64]*bytes.Buffer)
	for {
		select {
		case sm := <-n.messages:
			var buf *bytes.Buffer
			if b, ok := batches[sm.to]; !ok {
				buf = new(bytes.Buffer)
				batches[sm.to] = buf
			} else {
				buf = b
			}
			x.Check(binary.Write(buf, binary.LittleEndian, uint32(len(sm.data))))
			x.Check2(buf.Write(sm.data))

		default:
			start := time.Now()
			for to, buf := range batches {
				if buf.Len() == 0 {
					continue
				}
				data := make([]byte, buf.Len())
				copy(data, buf.Bytes())
				go n.doSendMessage(to, data)
				buf.Reset()
			}
			// Add a sleep clause to avoid a busy wait loop if there's no input to commitCh.
			sleepFor := 10*time.Millisecond - time.Since(start)
			time.Sleep(sleepFor)
		}
	}
}

func (n *node) doSendMessage(to uint64, data []byte) {
	ctx, cancel := context.WithTimeout(context.Background(), time.Second)
	defer cancel()

	addr := n.peers.Get(to)
	if len(addr) == 0 {
		return
	}
	pool := pools().get(addr)
	conn, err := pool.Get()
	x.Check(err)
	defer pool.Put(conn)

	c := protos.NewWorkerClient(conn)
	p := &protos.Payload{Data: data}

	ch := make(chan error, 1)
	go func() {
		_, err = c.RaftMessage(ctx, p)
		ch <- err
	}()

	select {
	case <-ctx.Done():
		return
	case err := <-ch:
		if grpc.ErrorDesc(err) == errorNodeIDBanned.Error() {
			groups().syncMemberships()
			groups().removeNode(n.gid, *raftId)
		}
		// We don't need to do anything if we receive any error while sending message.
		// RAFT would automatically retry.
		return
	}
}

func (n *node) processMutation(e raftpb.Entry, m *protos.Mutations) error {
	// TODO: Need to pass node and entry index.
	rv := x.RaftValue{Group: n.gid, Index: e.Index}
	ctx := context.WithValue(n.ctx, "raft", rv)
	if err := runMutations(ctx, m.Edges); err != nil {
		x.TraceError(n.ctx, err)
		return err
	}
	return nil
}

func (n *node) processSchemaMutations(e raftpb.Entry, m *protos.Mutations) error {
	// TODO: Need to pass node and entry index.
	rv := x.RaftValue{Group: n.gid, Index: e.Index}
	ctx := context.WithValue(n.ctx, "raft", rv)
	if err := runSchemaMutations(ctx, m.Schema); err != nil {
		x.TraceError(n.ctx, err)
		return err
	}
	return nil
}

func (n *node) processMembership(e raftpb.Entry, mm *protos.Membership) error {
	x.AssertTrue(n.gid == 0)
	groups().applyMembershipUpdate(e.Index, mm)
	return nil
}

func (n *node) process(e raftpb.Entry, pending chan struct{}) {
	defer func() {
		n.applied.Ch <- x.Mark{Index: e.Index, Done: true}
		posting.SyncMarkFor(n.gid).Ch <- x.Mark{Index: e.Index, Done: true}
	}()

	if e.Type != raftpb.EntryNormal {
		return
	}

	pending <- struct{}{} // This will block until we can write to it.
	var proposal protos.Proposal
	x.AssertTrue(len(e.Data) > 0)
	x.Checkf(proposal.Unmarshal(e.Data[1:]), "Unable to parse entry: %+v", e)

	var err error
	if proposal.Mutations != nil {
		err = n.processMutation(e, proposal.Mutations)
	} else if proposal.Membership != nil {
		err = n.processMembership(e, proposal.Membership)
	}
	n.props.Done(proposal.Id, err)
	<-pending // Release one.
}

const numPendingMutations = 10000

func (n *node) processApplyCh() {
	pending := make(chan struct{}, numPendingMutations)

	for e := range n.applyCh {
		mark := x.Mark{Index: e.Index, Done: true}

		if len(e.Data) == 0 {
			n.applied.Ch <- mark
			posting.SyncMarkFor(n.gid).Ch <- mark
			continue
		}

		if e.Type == raftpb.EntryConfChange {
			var cc raftpb.ConfChange
			var err error
			cc.Unmarshal(e.Data)

			if cc.Type == raftpb.ConfChangeRemoveNode {
				if n.numPeers() == 0 {
					err = errorNoPeer
				} else {
					cs := n.Raft().ApplyConfChange(cc)
					n.SetConfState(cs)
					// The following check ensures that node doesn't remove
					// itself during replay of logs when we want to add
					// the node back
					if cc.NodeID == *raftId {
						// sync memberships
						groups().syncMemberships()
					}
					if groups().reject(n.gid, cc.NodeID) {
						groups().removeNode(n.gid, cc.NodeID)
					}
				}
			} else if len(cc.Context) > 0 {
				var rc protos.RaftContext
				x.Check(rc.Unmarshal(cc.Context))
				// check for removed node id
				if groups().reject(rc.Group, rc.Id) {
					err = errorNodeIDBanned
				} else {
					n.Connect(rc.Id, rc.Addr)
					cs := n.Raft().ApplyConfChange(cc)
					n.SetConfState(cs)
				}
			} else {
				cs := n.Raft().ApplyConfChange(cc)
				n.SetConfState(cs)
			}

			n.props.Done(cc.ID, err)
			n.applied.Ch <- mark
			posting.SyncMarkFor(n.gid).Ch <- mark
			continue
		}

		// We derive the schema here if it's not present
		// Since raft committed logs are serialized, we can derive
		// schema here without any locking
		var proposal protos.Proposal
		x.Checkf(proposal.Unmarshal(e.Data[1:]), "Unable to parse entry: %+v", e)

		if e.Type == raftpb.EntryNormal && proposal.Mutations != nil {
			// process schema mutations before
			if proposal.Mutations.Schema != nil {
				// Wait for applied watermark to reach till previous index
				// All mutations before this should use old schema and after this
				// should use new schema
				n.waitForAppliedMark(n.ctx, e.Index-1)
				if err := n.processSchemaMutations(e, proposal.Mutations); err != nil {
					n.applied.Ch <- mark
					posting.SyncMarkFor(n.gid).Ch <- mark
					n.props.Done(proposal.Id, err)
					continue
				}
			}

			// stores a map of predicate and type of first mutation for each predicate
			schemaMap := make(map[string]types.TypeID)
			for _, edge := range proposal.Mutations.Edges {
				if _, ok := schemaMap[edge.Attr]; !ok {
					schemaMap[edge.Attr] = posting.TypeID(edge)
				}
			}

			for attr, storageType := range schemaMap {
				if _, err := schema.State().TypeOf(attr); err != nil {
					// Schema doesn't exist
					// Since committed entries are serialized, updateSchemaIfMissing is not
					// needed, In future if schema needs to be changed, it would flow through
					// raft so there won't be race conditions between read and update schema
					updateSchemaType(attr, storageType, e.Index, n.raftContext.Group)
				}
			}
		}

		go n.process(e, pending)
	}
}

func (n *node) saveToStorage(s raftpb.Snapshot, h raftpb.HardState,
	es []raftpb.Entry) {
	if !raft.IsEmptySnap(s) {
		le, err := n.store.LastIndex()
		if err != nil {
			log.Fatalf("While retrieving last index: %v\n", err)
		}
		if s.Metadata.Index <= le {
			return
		}

		if err := n.store.ApplySnapshot(s); err != nil {
			log.Fatalf("Applying snapshot: %v", err)
		}
		cs := s.Metadata.ConfState
		n.SetConfState(&cs)
	}

	if !raft.IsEmptyHardState(h) {
		n.store.SetHardState(h)
	}
	n.store.Append(es)
}

func (n *node) retrieveSnapshot(rc protos.RaftContext) {
	addr := n.peers.Get(rc.Id)
	x.AssertTruef(addr != "", "Should have the address for %d", rc.Id)
	pool := pools().get(addr)
	x.AssertTruef(pool != nil, "Pool shouldn't be nil for address: %v for id: %v", addr, rc.Id)

	x.AssertTrue(rc.Group == n.gid)
	// Get index of last committed.
	lastIndex, err := n.store.LastIndex()
	x.Checkf(err, "Error while getting last index")
	// Wait for watermarks to sync since populateShard writes directly to db, otherwise
	// the values might get overwritten
	// Safe to keep this line
	n.syncAllMarks(n.ctx, lastIndex)
	// Need to clear pl's stored in memory for the case when retrieving snapshot with
	// index greater than this node's last index
	// Should invalidate/remove pl's to this group only ideally
	posting.EvictGroup(n.gid)
	x.Check2(populateShard(n.ctx, pool, n.gid))
	// Populate shard stores the streamed data directly into db, so we need to refresh
	// schema for current group id
	x.Checkf(schema.LoadFromDb(n.gid), "Error while initilizating schema")
}

func (n *node) Run() {
	firstRun := true
	var leader bool
	ticker := time.NewTicker(time.Second)
	defer ticker.Stop()
	rcBytes, err := n.raftContext.Marshal()
	x.Check(err)
	for {
		select {
		case <-ticker.C:
			n.Raft().Tick()

		case rd := <-n.Raft().Ready():
			if rd.SoftState != nil {
				if rd.RaftState == raft.StateFollower && leader {
					// stepped down as leader do a sync membership immediately
					go groups().syncMemberships()
				} else if rd.RaftState == raft.StateLeader && !leader {
					// TODO:wait for apply watermark ??
					leaseMgr().resetLease(n.gid)
					go groups().syncMemberships()
				}
				leader = rd.RaftState == raft.StateLeader
			}
			x.Check(n.wal.StoreSnapshot(n.gid, rd.Snapshot))
			x.Check(n.wal.Store(n.gid, rd.HardState, rd.Entries))

			n.saveToStorage(rd.Snapshot, rd.HardState, rd.Entries)

			for _, msg := range rd.Messages {
				// NOTE: We can do some optimizations here to drop messages.
				msg.Context = rcBytes
				n.send(msg)
			}

			if !raft.IsEmptySnap(rd.Snapshot) {
				// We don't send snapshots to other nodes. But, if we get one, that means
				// either the leader is trying to bring us up to state; or this is the
				// snapshot that I created. Only the former case should be handled.
				var rc protos.RaftContext
				x.Check(rc.Unmarshal(rd.Snapshot.Data))
				if rc.Id != n.id {
					fmt.Printf("-------> SNAPSHOT [%d] from %d\n", n.gid, rc.Id)
					n.retrieveSnapshot(rc)
					fmt.Printf("-------> SNAPSHOT [%d]. DONE.\n", n.gid)
				} else {
					fmt.Printf("-------> SNAPSHOT [%d] from %d [SELF]. Ignoring.\n", n.gid, rc.Id)
				}
			}
			if len(rd.CommittedEntries) > 0 {
				x.Trace(n.ctx, "Found %d committed entries", len(rd.CommittedEntries))
			}

			for _, entry := range rd.CommittedEntries {
				// Need applied watermarks for schema mutation also for read linearazibility
				// Applied watermarks needs to be emitted as soon as possible sequentially.
				// If we emit Mark{4, false} and Mark{4, true} before emitting Mark{3, false}
				// then doneUntil would be set as 4 as soon as Mark{4,true} is done and before
				// Mark{3, false} is emitted. So it's safer to emit watermarks as soon as
				// possible sequentially
				status := x.Mark{Index: entry.Index, Done: false}
				n.applied.Ch <- status
				posting.SyncMarkFor(n.gid).Ch <- status

				// Just queue up to be processed. Don't wait on them.
				n.applyCh <- entry
			}

			n.Raft().Advance()
			if firstRun && n.canCampaign {
				go n.Raft().Campaign(n.ctx)
				firstRun = false
			}

		case <-n.stop:
			if peerId, has := groups().Peer(n.gid, *raftId); has && n.AmLeader() {
				n.Raft().TransferLeadership(n.ctx, *raftId, peerId)
				go func() {
					select {
					case <-n.ctx.Done(): // time out
						if n.AmLeader() {
							x.Trace(n.ctx, "context timed out while transfering leadership")
						}
					case <-time.After(1 * time.Second):
						if n.AmLeader() {
							x.Trace(n.ctx, "Timed out transfering leadership")
						}
					}
					n.Raft().Stop()
					close(n.done)
				}()
			} else {
				n.Raft().Stop()
				close(n.done)
			}
		case <-n.done:
			return
		}
	}
}

func (n *node) Stop() {
	select {
	case n.stop <- struct{}{}:
	case <-n.done:
		// already stopped.
		return
	}
	<-n.done // wait for Run to respond.
}

func (n *node) Step(ctx context.Context, msg raftpb.Message) error {
	return n.Raft().Step(ctx, msg)
}

func (n *node) checkMembership() {
	ticker := time.NewTicker(10 * time.Second)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			// Check if node is still present even after being removed
			// in membership configuration
			if !n.AmLeader() {
				continue
			}

			for _, id := range n.raftPeers() {
				if groups().reject(n.gid, id) {
					stopGroup(n.ctx, n.gid, id)
				}
			}
		case <-n.done:
			return
		}
	}
}

func (n *node) snapshotPeriodically() {
	if n.gid == 0 {
		// Group zero is dedicated for membership information, whose state we don't persist.
		// So, taking snapshots would end up deleting the RAFT entries that we need to
		// regenerate the state on a crash. Therefore, don't take snapshots.
		return
	}

	ticker := time.NewTicker(time.Minute)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			n.snapshot(*maxPendingCount)

		case <-n.done:
			return
		}
	}
}

func (n *node) snapshot(skip uint64) {
	if n.gid == 0 {
		// Group zero is dedicated for membership information, whose state we don't persist.
		// So, taking snapshots would end up deleting the RAFT entries that we need to
		// regenerate the state on a crash. Therefore, don't take snapshots.
		return
	}
	water := posting.SyncMarkFor(n.gid)
	le := water.DoneUntil()

	existing, err := n.store.Snapshot()
	x.Checkf(err, "Unable to get existing snapshot")

	si := existing.Metadata.Index
	if le <= si+skip {
		return
	}
	snapshotIdx := le - skip
	x.Trace(n.ctx, "Taking snapshot for group: %d at watermark: %d\n", n.gid, snapshotIdx)
	rc, err := n.raftContext.Marshal()
	x.Check(err)

	s, err := n.store.CreateSnapshot(snapshotIdx, n.ConfState(), rc)
	x.Checkf(err, "While creating snapshot")
	x.Checkf(n.store.Compact(snapshotIdx), "While compacting snapshot")
	x.Check(n.wal.StoreSnapshot(n.gid, s))
}

func (n *node) joinPeers() {
	// Get leader information for MY group.
	pid, paddr := groups().Leader(n.gid)
	if pid == *raftId {
		return
	}
	n.Connect(pid, paddr)
	fmt.Printf("joinPeers connected with: %q with peer id: %d\n", paddr, pid)

	pool := pools().get(paddr)
	x.AssertTruef(pool != nil, "Unable to get pool for addr: %q for peer: %d", paddr, pid)

	// Bring the instance up to speed first.
	// Raft would decide whether snapshot needs to fetched or not
	// so populateShard is not needed
	// _, err := populateShard(n.ctx, pool, n.gid)
	// x.Checkf(err, "Error while populating shard")

	conn, err := pool.Get()
	x.Check(err)
	defer pool.Put(conn)

	c := protos.NewWorkerClient(conn)
	x.Printf("Calling JoinCluster")
	cc := &protos.ClusterConfChange{
		Context: n.raftContext,
		Op:      protos.ClusterConfChange_JOIN,
	}
	_, err = c.ManageCluster(n.ctx, cc)
	x.Checkf(err, "Error while joining cluster")
	x.Printf("Done with JoinCluster call\n")
}

func (n *node) hasID(nid uint64) bool {
	n.RLock()
	defer n.RUnlock()
	for _, id := range n.ConfState().Nodes {
		if id == nid {
			return true
		}
	}
	return false
}

func (n *node) raftPeers() (peers []uint64) {
	n.RLock()
	defer n.RUnlock()
	for _, id := range n.ConfState().Nodes {
		peers = append(peers, id)
	}
	return
}

func (n *node) numPeers() int {
	n.RLock()
	defer n.RUnlock()
	return len(n._confState.Nodes) - 1
}

func (n *node) initFromWal(wal *raftwal.Wal) (restart bool, rerr error) {
	n.wal = wal

	var sp raftpb.Snapshot
	sp, rerr = wal.Snapshot(n.gid)
	if rerr != nil {
		return
	}
	var term, idx uint64
	if !raft.IsEmptySnap(sp) {
		fmt.Printf("Found Snapshot: %+v\n", sp)
		restart = true
		if rerr = n.store.ApplySnapshot(sp); rerr != nil {
			return
		}
		// copy
		cs := sp.Metadata.ConfState
		n.SetConfState(&cs)
		term = sp.Metadata.Term
		idx = sp.Metadata.Index
		fmt.Printf("setting done until %v\n", idx)
		posting.SyncMarkFor(n.gid).SetDoneUntil(idx)
	}

	var hd raftpb.HardState
	hd, rerr = wal.HardState(n.gid)
	if rerr != nil {
		return
	}
	if !raft.IsEmptyHardState(hd) {
		fmt.Printf("Found hardstate: %+v\n", sp)
		restart = true
		if rerr = n.store.SetHardState(hd); rerr != nil {
			return
		}
	}

	var es []raftpb.Entry
	es, rerr = wal.Entries(n.gid, term, idx)
	if rerr != nil {
		return
	}
	fmt.Printf("Group %d found %d entries\n", n.gid, len(es))
	if len(es) > 0 {
		restart = true
	}
	rerr = n.store.Append(es)
	return
}

// InitAndStartNode gets called after having at least one membership sync with the cluster.
func (n *node) InitAndStartNode(wal *raftwal.Wal) {
	restart, err := n.initFromWal(wal)
	x.Check(err)

	if restart {
		fmt.Printf("Restarting node for group: %d\n", n.gid)
		n.SetRaft(raft.RestartNode(n.cfg))

	} else {
		fmt.Printf("New Node for group: %d\n", n.gid)
		if groups().HasPeer(n.gid) {
			n.joinPeers()
			n.SetRaft(raft.StartNode(n.cfg, nil))

		} else {
			peers := []raft.Peer{{ID: n.id}}
			n.SetRaft(raft.StartNode(n.cfg, peers))
			// Trigger election, so this node can become the leader of this single-node cluster.
			n.canCampaign = true
		}
	}
	go n.processApplyCh()
	go n.Run()
	// TODO: Find a better way to snapshot, so we don't lose the membership
	// state information, which isn't persisted.
	go n.snapshotPeriodically()
	go n.batchAndSendMessages()
	go n.checkMembership()
}

func (n *node) AmLeader() bool {
	if n.Raft() == nil {
		return false
	}
	r := n.Raft()
	return r.Status().Lead == r.Status().ID
}

func (w *grpcWorker) applyMessage(ctx context.Context, msg raftpb.Message) error {
	var rc protos.RaftContext
	x.Check(rc.Unmarshal(msg.Context))
	node := groups().Node(rc.Group)
	if node == nil {
		return errorStoppedServingGroup
	}
	// Don't reject messages until the node is not removed
	if groups().reject(rc.Group, msg.From) && !node.hasID(msg.From) {
		return errorNodeIDBanned
	}
	node.Connect(msg.From, rc.Addr)

	c := make(chan error, 1)
	go func() { c <- node.Step(ctx, msg) }()

	select {
	case <-ctx.Done():
		return ctx.Err()
	case err := <-c:
		return err
	}
}

func (w *grpcWorker) RaftMessage(ctx context.Context, query *protos.Payload) (*protos.Payload, error) {
	if ctx.Err() != nil {
		return &protos.Payload{}, ctx.Err()
	}

	for idx := 0; idx < len(query.Data); {
		x.AssertTruef(len(query.Data[idx:]) >= 4,
			"Slice left of size: %v. Expected at least 4.", len(query.Data[idx:]))

		sz := int(binary.LittleEndian.Uint32(query.Data[idx : idx+4]))
		idx += 4
		msg := raftpb.Message{}
		if idx+sz-1 > len(query.Data) {
			return &protos.Payload{}, x.Errorf(
				"Invalid query. Size specified: %v. Size of array: %v\n", sz, len(query.Data))
		}
		if err := msg.Unmarshal(query.Data[idx : idx+sz]); err != nil {
			x.Check(err)
		}
		if msg.To != *raftId {
			return &protos.Payload{}, errorInvalidToNodeId
		}

		if msg.Type != raftpb.MsgHeartbeat && msg.Type != raftpb.MsgHeartbeatResp {
			fmt.Printf("RECEIVED: %v %v-->%v\n", msg.Type, msg.From, msg.To)
		}
		if err := w.applyMessage(ctx, msg); err != nil {
			return &protos.Payload{}, err
		}
		idx += sz
	}
	// fmt.Printf("Got %d messages\n", count)
	return &protos.Payload{}, nil
}

func startGroup(gid uint32) error {
	if !groups().ServesGroup(gid) {
		node := groups().newNode(uint32(gid), *raftId, *myAddr)
		err := schema.LoadFromDb(uint32(gid))
		x.Checkf(err, "Schema load for group %d failed", gid)
		// Reset watermark
		posting.SyncMarkFor(gid).SetDoneUntil(0)
		node.InitAndStartNode(gr.wal)
	}
	_, has := groups().Peer(gid, *raftId)
	if has {
		groups().Node(gid).joinPeers()
	}
	return nil
}

func stopGroup(ctx context.Context, gid uint32, nodeId uint64) error {
	node := groups().Node(gid)
	if node == nil {
		return nil
	}
	if node.numPeers() == 0 {
		// For single node group just stop them
		groups().removeNode(gid, nodeId)
		return nil
	}
	if err := node.RemoveFromCluster(ctx, nodeId); err != nil {
		return err
	}
	return nil
}

func manageCluster(ctx context.Context, cc *protos.ClusterConfChange) error {
	switch cc.Op {
	case protos.ClusterConfChange_ADD:
		x.AssertTrue(cc.Context.Id == *raftId)
		return startGroup(cc.Context.Group)
	case protos.ClusterConfChange_REMOVE:
		return stopGroup(ctx, cc.Context.Group, cc.Context.Id)
	case protos.ClusterConfChange_JOIN:
		return joinCluster(ctx, cc.Context)
	default:
		x.Fatalf("Unknown cluster conf change Operation")
		return nil
	}
}

func (w *grpcWorker) ManageCluster(ctx context.Context, cc *protos.ClusterConfChange) (*protos.Payload, error) {
	if ctx.Err() != nil {
		return &protos.Payload{}, ctx.Err()
	}

	c := make(chan error, 1)
	go func() { c <- manageCluster(ctx, cc) }()

	select {
	case <-ctx.Done():
		return &protos.Payload{}, ctx.Err()
	case err := <-c:
		return &protos.Payload{}, err
	}
}

func joinCluster(ctx context.Context, rc *protos.RaftContext) error {
	s, found := groups().Server(rc.Id, rc.Group)
	if rc.Id == *raftId || (s.Addr == rc.Addr && found) {
		return errorNodeIDExists
	}

	node := groups().Node(rc.Group)
	if node == nil {
		return nil
	}
	node.Connect(rc.Id, rc.Addr)

	err := node.AddToCluster(ctx, rc.Id)
	return err
}

func canServeLocally(gid uint32, nodeId uint64, op clusterOp) bool {
	switch op {
	case ADD_GROUPS:
		return nodeId == *raftId
	default:
		return groups().ServesGroup(gid)
	}
}

func toMembershipChange(gid uint32, nodeId uint64, op clusterOp) *protos.ClusterConfChange {
	rc := &protos.RaftContext{Id: nodeId, Group: gid}
	cc := &protos.ClusterConfChange{Context: rc}
	if op == ADD_GROUPS {
		cc.Op = protos.ClusterConfChange_ADD
	} else if op == REMOVE_GROUPS || op == REMOVE_SERVER {
		cc.Op = protos.ClusterConfChange_REMOVE
	} else {
		x.Fatalf("Unknown membership change")
	}
	return cc
}

func manageClusterOverNetwork(ctx context.Context, gid uint32, nodeId uint64, op clusterOp) error {
	mc := toMembershipChange(gid, nodeId, op)
	if canServeLocally(gid, nodeId, op) {
		if err := manageCluster(ctx, mc); err != nil {
			return err
		}
		return nil
	}

	var paddr string
	var pid uint64
	if op == ADD_GROUPS {
		paddr = groups().address(nodeId)
		if len(paddr) == 0 {
			return x.Errorf("Node with id %d not found", nodeId)
		}
		pid = nodeId
	} else {
		pid, paddr = groups().Leader(gid)
		if len(paddr) == 0 {
			return x.Errorf("Leader not found for group %d", gid)
		}
	}

	pools().connect(paddr)
	fmt.Printf("Manage Cluster connected with: %q with node id: %d\n", paddr, pid)

	pool := pools().get(paddr)
	x.AssertTruef(pool != nil, "Unable to get pool for addr: %q for peer: %d", paddr, pid)

	conn, err := pool.Get()
	x.Check(err)
	defer pool.Put(conn)

	c := protos.NewWorkerClient(conn)
	x.Printf("Calling Manage Cluster")
	_, err = c.ManageCluster(ctx, mc)
	if err != nil {
		return err
	}
	return nil
}

func ManageClusterOverNetwork(mc ClusterConfChanges) error {
	addr := groups().address(mc.Id)
	if len(addr) == 0 {
		return x.Errorf(
			"Node with id %d not found, please retry the request on node %d",
			mc.Id, mc.Id)
	}
	if mc.Op == REMOVE_SERVER {
		if len(mc.Gids) == 0 {
			mc.Gids = groups().groupsServed(mc.Id)
		}
		groups().syncBannedId(mc.Id, addr)
	}

	for _, gid := range mc.Gids {
		if mc.Op == ADD_GROUPS {
			if _, found := groups().Server(mc.Id, gid); found {
				return x.Errorf("Node %d is already serving group %d", mc.Id, gid)
			}
			groups().syncClearRemovedId(gid, mc.Id, addr)
		} else if mc.Op == REMOVE_GROUPS {
			if _, found := groups().Server(mc.Id, gid); !found {
				return x.Errorf("node %d not serving group %d", mc.Id, gid)
			}
			groups().syncRemovedId(gid, mc.Id, addr)
		}

		if err := manageClusterOverNetwork(context.Background(), gid, mc.Id, mc.Op); err != nil {
			return err
		}
	}
	return nil
}
